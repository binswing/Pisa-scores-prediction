# -*- coding: utf-8 -*-
"""PISA code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JahXc7-oO4uVPD2l0u-_pB3TYF6rAtnV
"""

import numpy as np # linear algebra
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
plt.style.use('seaborn-v0_8-whitegrid')

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 1000)
warnings.filterwarnings("ignore")

df = pd.read_csv('../dataset/economics_and_education_dataset_CSV.csv')

"""#Data preprocessing"""

df = df.rename(columns={'expenditure_on _education_pct_gdp': 'expenditure_on_education_pct_gdp'})

df = df.drop(columns=["index_code","alcohol_consumption_per_capita"])
df.head()

"""###Label encode"""

df = pd.get_dummies(df, columns = ['sex'])
df['sex_BOY'] = df['sex_BOY'].apply(lambda x : 1 if x == True else 0)
df['sex_GIRL'] = df['sex_GIRL'].apply(lambda x : 1 if x == True else 0)
df['sex_TOT'] = df['sex_TOT'].apply(lambda x : 1 if x == True else 0)
df.head()

from sklearn.preprocessing import StandardScaler, LabelEncoder
NUM = df.select_dtypes(include=['number']).columns
CAT = df.select_dtypes(include=['object']).columns
for col in CAT:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))

"""##Imputting"""

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=7)
NUM_imputed = pd.DataFrame(
    imputer.fit_transform(df[NUM]),
    columns=NUM,
    index=df.index
)
df_imputed = pd.concat([NUM_imputed, df[CAT]], axis=1)

"""#Data processing"""

from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression

y = df_imputed['rating']
X = df_imputed.drop(columns=['rating'])
model = LinearRegression()

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
def eval_metrics(y_test, y_pred):
  mae = mean_absolute_error(y_test, y_pred)
  print(f"Mean Absolute Error (MAE): {mae:.4f}")
  mse = mean_squared_error(y_test, y_pred)
  print(f"Mean Squared Error (MSE): {mse:.4f}")
  rmse = np.sqrt(mse)
  print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
  r2 = r2_score(y_test, y_pred)
  print(f"R squared score: {r2:.4f}")

"""##Train trest split"""

def rating_group(val):
    if val <= 357:
        return 1
    if val > 357 and val <= 410:
        return 2
    elif val > 410 and val <= 442:
        return 3
    elif val > 442 and val <= 460:
        return 4
    elif val > 460 and val <= 480:
        return 5
    elif val > 480 and val <= 508:
        return 6
    elif val > 508 and val <= 520:
        return 7
    elif val > 520 and val <= 540:
        return 8
    else:
        return 9
df_imputed['rating_groups'] = df_imputed['rating'].apply(rating_group)

from sklearn.model_selection import train_test_split
stratify_base = df_imputed['rating_groups']
y = df_imputed['rating']
X = df_imputed.drop(columns=['rating','rating_groups'])

#train:test = 94:6
X_train, X_test, y_train, y_test = train_test_split(X,y,
                                   random_state=104,
                                   test_size=0.06,
                                   stratify=stratify_base)

"""##Rating prediction with XGBoost"""

import xgboost as xgb
xgbmodel = xgb.XGBRegressor(objective='reg:squarederror',
                         n_estimators=500,
                         learning_rate=0.1,
                         max_depth=12,
                         max_leaves=16,
                         reg_alpha=0,
                         reg_lambda=1,
                         random_state=42)
xgbmodel.fit(X_train, y_train)

xgb_y_pred = xgbmodel.predict(X_test)
print("XGBoost Regression result")
eval_metrics(y_test, xgb_y_pred)
print("-------------------------------------------")

"""##Rating prediction with LightGBM"""

import lightgbm as lgb
lgbmodel = lgb.LGBMRegressor(objective='regression',
                             metric='mae',
                             n_estimators=500,
                             learning_rate=0.1,
                             max_depth=12,
                             num_leaves=16,
                             min_child_samples=10,
                             reg_alpha=0,
                             reg_lambda=1,
                             verbose=-1,
                             random_state=42)

lgbmodel.fit(X_train, y_train)

lgb_y_pred = lgbmodel.predict(X_test)

print("LightGBM Regression result")

eval_metrics(y_test, lgb_y_pred)